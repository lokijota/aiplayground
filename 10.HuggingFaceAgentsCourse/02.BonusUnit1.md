# Hugging Face Agents Course - Notes and Code
23/Feb/2025

## Bonus Unit 1 - Fine-tune a Large Language Model (LLM) for function calling
https://huggingface.co/learn/agents-course/bonus-unit1/introduction

The idea is, rather than relying only on prompt-based approaches like we did in Unit 1, function calling trains your model to take actions and interpret observations during the training phase, making your AI more robust.

The best way for you to be able to follow this Bonus Unit is:
- Know how to Fine-Tune an LLM with Transformers, if it’s not the case check [this](https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt).
- Know how to use SFTTrainer to fine-tune our model, to learn more about it check [this documentation]().
https://huggingface.co/learn/nlp-course/en/chapter11/1

### What is Function Calling?
https://huggingface.co/learn/agents-course/bonus-unit1/what-is-function-calling

Function-calling is a way for an LLM to take actions on its environment. It was first introduced in GPT-4, and was later reproduced in other models.

Just like the tools of an Agent, function-calling gives the model the capacity to take an action on its environment. However, the function calling capacity is learned by the model, and relies less on prompting than other agents techniques.

During Unit 1, the Agent didn’t learn to use the Tools, we just provided the list, and we relied on the fact that the model was able to generalize on defining a plan using these Tools.

While here, with function-calling, the Agent is fine-tuned (trained) to use Tools.

#### How does the model “learn” to take an action?

In Unit 1, we explored the general workflow of an agent. Once the user has given some tools to the agent and prompted it with a query, the model will cycle through:

- Think : What action(s) do I need to take in order to fulfill the objective.
- Act : Format the action with the correct parameter and stop the generation.
- Observe : Get back the result from the execution.

In a “typical” conversation with a model through an API, the conversation will alternate between user and assistant messages like this:

```json
conversation = [
    {"role": "user", "content": "I need help with my order"},
    {"role": "assistant", "content": "I'd be happy to help. Could you provide your order number?"},
    {"role": "user", "content": "It's ORDER-123"},
]
```

Function-calling brings new roles to the conversation!
- One new role for an Action
- One new role for an Observation

If we take the [Mistral API](https://docs.mistral.ai/capabilities/function_calling/) as an example, it would look like this:

```json
conversation = [
    {
        "role": "user",
        "content": "What's the status of my transaction T1001?"
    },
    {
        "role": "assistant",
        "content": "",
        "function_call": {
            "name": "retrieve_payment_status",
            "arguments": "{\"transaction_id\": \"T1001\"}"
        }
    },
    {
        "role": "tool",
        "name": "retrieve_payment_status",
        "content": "{\"status\": \"Paid\"}"
    },
    {
        "role": "assistant",
        "content": "Your transaction T1001 has been successfully paid."
    }
]
```

> *...But you said there’s a new role for function calls?*
 
Yes and no, in this case and in a lot of other APIs, the model formats the action to take as an “assistant” message. The chat template will then represent this as special tokens for function-calling:

- `[AVAILABLE_TOOLS]` – Start the list of available tools
- `[/AVAILABLE_TOOLS]` – End the list of available tools
- `[TOOL_CALLS]` – Make a call to a tool (i.e., take an “Action”)
- `[TOOL_RESULTS]` – “Observe” the result of the action
- `[/TOOL_RESULTS]` – End of the observation (i.e., the model can decode again)

We’ll talk again about function-calling in this course, but if you want to dive deeper you can check this [excellent documentation section](https://docs.mistral.ai/capabilities/function_calling/).

Now that we learned what function-calling is and how it works, let’s add some function-calling capabilities to a model that does not have those capacities yet: `google/gemma-2-2b-it`, by appending some new special tokens to the model.

To be able to do that, we need first to understand fine-tuning and LoRA.

### Let’s Fine-Tune Your Model for Function-Calling

How do we train our model for function-calling?
> Answer: We need data

A model training process can be divided into 3 steps:

1. The model is **pre-trained** on a large quantity of data. The output of that step is a pre-trained model. For instance, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). It’s a base model and only knows how to predict the next token without strong instruction following capabilities.

2. To be useful in a chat context, the model then needs to be **fine-tuned to follow instructions**. In this step, it can be trained by model creators, the open-source community, you, or anyone. For instance, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) is an instruction-tuned model by the Google Team behind the Gemma project.

3. The model can then be **aligned to the creator’s preferences**. For instance, a customer service chat model that must never be impolite to customers.

In this tutorial, we will build a function-calling model based on `google/gemma-2-2b-it`. We choose the fine-tuned model google/gemma-2-2b-it instead of the base model `google/gemma-2-2b` because the fine-tuned model has been improved for our use-case.

Starting from the pre-trained model would require more training in order to learn instruction following, chat AND function-calling.

By starting from the instruction-tuned model, we minimize the amount of information that our model needs to learn.

#### LoRA (Low-Rank Adaptation of Large Language Models)

LoRA is a popular and lightweight training technique that significantly reduces the number of trainable parameters.

It works by inserting a smaller number of new weights as an adapter into the model to train. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share.

![LORA](blog_multi-lora-serving_LoRA.gif)

LoRA works by adding pairs of rank decomposition matrices to Transformer layers, typically focusing on linear layers. During training, we will “freeze” the rest of the model and will only update the weights of those newly added adapters.

By doing so, the number of parameters that we need to train drops considerably as we only need to update the adapter’s weights.

During inference, the input is passed into the adapter and the base model, or these adapter weights can be merged with the base model, resulting in no additional latency overhead.

LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable. This helps reduce the memory required to train a model.

If you want to learn more about how LoRA works, you should check out this [tutorial](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).


#### Fine-Tuning a Model for Function-Calling

[Notebook here](10.HuggingFaceAgentsCourse/bonus-unit1.ipynb) or in [Colab](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).


My notebook on colab: https://colab.research.google.com/drive/1ynsp7LYzr0IvNBMZHU1p58Y893Z_a61r#scrollTo=X6DBY8AqxFLL

Note: the code takes 6-7hrs to run in Colab without A100's, and doesn't run locally on the Mac with the latest version of PyTorch. The fix is supposedly already checked-in, but it doesn't seem to be released yet.